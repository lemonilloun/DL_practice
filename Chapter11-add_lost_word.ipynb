{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-08-05T20:50:59.204818Z",
     "start_time": "2023-08-05T20:50:58.576789Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, random, math, numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random.seed(1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T20:50:59.212009Z",
     "start_time": "2023-08-05T20:50:59.205603Z"
    }
   },
   "id": "17fdedbd0d4772b2"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "f = open('reviews.txt')\n",
    "raw_reviews = f.readlines()\n",
    "f.close()\n",
    "\n",
    "tokens = list(map(lambda x:(x.split(\" \")), raw_reviews))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T20:50:59.709991Z",
     "start_time": "2023-08-05T20:50:59.208969Z"
    }
   },
   "id": "c6c8460daf51226b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "wordcnt = Counter()\n",
    "for sent in tokens:\n",
    "    for word in sent:\n",
    "        wordcnt[word] -= 1\n",
    "vocab = list(set(map(lambda  x:x[0], wordcnt.most_common())))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T20:51:01.231113Z",
     "start_time": "2023-08-05T20:50:59.709121Z"
    }
   },
   "id": "ae3ce2f558d8111"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "for i,word in enumerate(vocab):\n",
    "    word2index[word] = i\n",
    "\n",
    "concatenated = list()\n",
    "input_dataset = list()\n",
    "\n",
    "for sent in tokens:\n",
    "    sent_indices = list()\n",
    "    for word in sent:\n",
    "        try:\n",
    "            sent_indices.append(word2index[word])\n",
    "            concatenated.append(word2index[word])\n",
    "        except:\n",
    "            \"\"\n",
    "    input_dataset.append(sent_indices)\n",
    "concatenated = np.array(concatenated)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T20:51:03.324575Z",
     "start_time": "2023-08-05T20:51:01.231871Z"
    }
   },
   "id": "be39c5e25938b9cd"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "random.shuffle(input_dataset)\n",
    "alpha, iterations = (0.05, 2)\n",
    "hidden_size, window, negative = (50, 2, 5)\n",
    "\n",
    "weights_0_1 = (np.random.rand(len(vocab),hidden_size) - 0.5) * 0.2\n",
    "weights_1_2 = np.random.rand(len(vocab),hidden_size)*0\n",
    "\n",
    "layer_2_target = np.zeros(negative + 1)\n",
    "layer_2_target[0] = 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T20:53:49.963701Z",
     "start_time": "2023-08-05T20:53:49.911410Z"
    }
   },
   "id": "c54d3219b287df9b"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def similar(target):\n",
    "    target_index = word2index[target]\n",
    "    \n",
    "    scores = Counter()\n",
    "    for word, index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - weights_0_1[target_index]\n",
    "        squared_diff = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_diff))\n",
    "    return scores.most_common(10)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T20:53:50.550535Z",
     "start_time": "2023-08-05T20:53:50.547505Z"
    }
   },
   "id": "d17357aa372a95d1"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:0.995   [('terrible', -0.0), ('brilliant', -3.231460136015008), ('horrible', -3.2346887695740794), ('horrid', -3.5408636273187546), ('pathetic', -3.5541401536169843), ('horrendous', -3.6524130884730908), ('magnificent', -3.811352213496053), ('phenomenal', -3.8452778421704), ('ridiculous', -3.8874019027174644), ('fantastic', -3.9429357974454335)]])])][('terrible', -0.0), ('horrible', -3.092257358215906), ('brilliant', -3.1764110366375466), ('pathetic', -3.581886101235946), ('horrendous', -3.647495793922012), ('horrid', -3.6997439837621275), ('magnificent', -3.81403503987757), ('phenomenal', -3.826304134902037), ('dreadful', -3.9217193545849662), ('ridiculous', -3.9664913066611787)]\n"
     ]
    }
   ],
   "source": [
    "for rev_i, review in enumerate(input_dataset * iterations):\n",
    "    for target_i in range(len(review)):\n",
    "        \n",
    "        #прогнозирование случайного подмножество, потому что прогнозирование всего словаря слишком много требует вычислений\n",
    "        target_samples = [review[target_i]]+list(concatenated[(np.random.rand(negative)*len(concatenated)).astype('int').tolist()])\n",
    "        \n",
    "        left_context = review[max(0, target_i - window) : target_i]\n",
    "        right_context = review[target_i + 1: min(len(review), target_i + window)]\n",
    "        \n",
    "        layer_1 = np.mean(weights_0_1[left_context + right_context], axis=0)\n",
    "        layer_2 = sigmoid(layer_1.dot(weights_1_2[target_samples].T))\n",
    "        \n",
    "        layer_2_delta = layer_2 - layer_2_target\n",
    "        layer_1_delta = layer_2_delta.dot(weights_1_2[target_samples])\n",
    "        \n",
    "        weights_0_1[left_context + right_context] -= layer_1_delta * alpha\n",
    "        weights_1_2[target_samples] -= np.outer(layer_2_delta, layer_1) * alpha\n",
    "\n",
    "    if(rev_i % 250 == 0):\n",
    "        sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)\n",
    "                                                       *iterations)) + \"   \" + str(similar('terrible')))\n",
    "        sys.stdout.write('\\rProgress:'+str(rev_i/float(len(input_dataset)\n",
    "                                               *iterations)))\n",
    "print(similar('terrible'))\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T21:04:11.084961Z",
     "start_time": "2023-08-05T20:53:51.073500Z"
    }
   },
   "id": "74ddde476b772bfe"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('beautiful', -0.0), ('creepy', -3.133661637069066), ('lovely', -3.1829254616359295), ('glamorous', -3.3605481638865915), ('nightmarish', -3.4773418364229407), ('spooky', -3.499259048116791), ('drab', -3.5156567880709453), ('fantastic', -3.581630524320107), ('heartwarming', -3.6028708757731818), ('gorgeous', -3.6061419438561275)]\n"
     ]
    }
   ],
   "source": [
    "print(similar('beautiful'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T16:01:29.441954Z",
     "start_time": "2023-08-05T16:01:29.059287Z"
    }
   },
   "id": "e37bfde0649d9838"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('love', -0.0), ('adore', -5.076843711565643), ('dislike', -5.192422162802242), ('commend', -5.266630042401484), ('empathise', -5.450702780254202), ('stupidest', -5.460194766095104), ('prefer', -5.467468671024043), ('debate', -5.532838216355729), ('revisit', -5.539255461253388), ('hate', -5.543813951137705)]\n"
     ]
    }
   ],
   "source": [
    "print(similar('love'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T16:02:50.567296Z",
     "start_time": "2023-08-05T16:02:50.201636Z"
    }
   },
   "id": "98d06bcd1c47354"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "def analogy(positive=['terrible','good'],negative=['bad']):\n",
    "\n",
    "    norms = np.sum(weights_0_1 * weights_0_1,axis=1)\n",
    "    norms.resize(norms.shape[0],1)\n",
    "    \n",
    "    normed_weights = weights_0_1 * norms\n",
    "\n",
    "    query_vect = np.zeros(len(weights_0_1[0]))\n",
    "    for word in positive:\n",
    "        query_vect += normed_weights[word2index[word]]\n",
    "    for word in negative:\n",
    "        query_vect -= normed_weights[word2index[word]]\n",
    "\n",
    "    scores = Counter()\n",
    "    for word,index in word2index.items():\n",
    "        raw_difference = weights_0_1[index] - query_vect\n",
    "        squared_difference = raw_difference * raw_difference\n",
    "        scores[word] = -math.sqrt(sum(squared_difference))\n",
    "\n",
    "    return scores.most_common(10)[1:]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T16:19:43.470722Z",
     "start_time": "2023-08-05T16:19:43.466863Z"
    }
   },
   "id": "f6a39692cf8f0c6c"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[('\\n', -339.1801983652029),\n ('woman', -339.34415988120145),\n ('rest', -339.4230021929077),\n ('king', -339.99795859992366),\n ('none', -340.1038813514828),\n ('majority', -340.14557482974095),\n ('father', -340.266150647704),\n ('daughter', -340.32762805755857),\n ('depiction', -340.346613499002)]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy(['king', 'woman'], ['man'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T16:22:05.109777Z",
     "start_time": "2023-08-05T16:22:04.683438Z"
    }
   },
   "id": "179873efacea9dfb"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "norms = np.sum(weights_0_1 * weights_0_1, axis=1)\n",
    "norms.resize(norms.shape[0], 1)\n",
    "normed_weights = weights_0_1 * norms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T21:05:16.280887Z",
     "start_time": "2023-08-05T21:05:16.267635Z"
    }
   },
   "id": "2211500e2ee916c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "<b>make_sent_vect<b> - преобразование каждого отдельного обзора (списка слов) в векторное представление методом усреднения"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2bbc1366d7227f4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "функция <b>most_similar_reviews<b> запрашивает обзоры, наиболее похожие на заданный, выполняя скалярное произведение между вектором обзора на входе и векторами всех обзоров"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be20dc45a85f02d6"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def make_sent_vect(words):\n",
    "    indices = list(map(lambda x:word2index[x],filter(lambda x:x in word2index,words)))\n",
    "    return np.mean(normed_weights[indices],axis=0)\n",
    "\n",
    "def most_similar_reviews(review):\n",
    "    v = make_sent_vect(review)\n",
    "    scores = Counter()\n",
    "    for i,val in enumerate(reviews2vectors.dot(v)):\n",
    "        scores[i] = val\n",
    "    most_similar = list()\n",
    "\n",
    "    for idx,score in scores.most_common(3):\n",
    "        most_similar.append(raw_reviews[idx][0:40])\n",
    "    return most_similar"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T21:15:04.223015Z",
     "start_time": "2023-08-05T21:15:04.218313Z"
    }
   },
   "id": "6e7b45d348bbcddf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Результат векторного представления сохраняется в матрицу <b>reviews2vectors<b>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a968e91fa6d074a3"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "reviews2vectors = list()\n",
    "for review in tokens:\n",
    "    reviews2vectors.append(make_sent_vect(review))\n",
    "reviews2vectors = np.array(reviews2vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T21:08:59.426946Z",
     "start_time": "2023-08-05T21:08:56.912790Z"
    }
   },
   "id": "69995afe3b45f62c"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "['i am   and i hated this film its the wor',\n 'this was a very good movie i wished i co',\n 'i read the reviews before i watched this']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_reviews(['awful', 'boring'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-05T21:15:18.536689Z",
     "start_time": "2023-08-05T21:15:18.530416Z"
    }
   },
   "id": "27b1f250a63a2259"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d0146d4e0f14769c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
